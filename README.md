# Metadatadriven-ETL-with-kafka-and-pyspark
# ğŸ§  Metadata-Driven ETL Pipeline (PostgreSQL, SQL Server âœ MySQL, Snowflake)

## ğŸ“Œ Overview

This project implements a **metadata-driven ETL pipeline** that:
- Extracts data from multiple **source databases** (PostgreSQL, SQL Server)
- Uses **Debezium + Kafka** for real-time CDC (Change Data Capture)
- Applies **transformation logic** using metadata rules stored in PostgreSQL
- Loads data into **destination databases** (MySQL, Snowflake)
- Enables reporting and visualization with **Power BI**
- Is fully containerized using **Docker Compose**

---

## ğŸ› ï¸ Tech Stack

| Layer            | Tools / Technologies                          |
|------------------|-----------------------------------------------|
| ğŸ—ƒï¸ Source DBs     | PostgreSQL, SQL Server                        |
| ğŸ”„ CDC Engine     | Debezium                                      |
| ğŸ“¡ Messaging      | Apache Kafka + Zookeeper                      |
| ğŸ§  Metadata DB    | PostgreSQL (fraud rules / transformation logic) |
| ğŸ”§ Transformation | PySpark                                       |
| ğŸ“¥ Destinations   | MySQL, Snowflake                              |
| ğŸ¯ Orchestration  | Apache Airflow                                |
| ğŸ“Š BI Tool        | Power BI (connects to MySQL/Snowflake)        |
| ğŸ“¦ Containerization| Docker + Docker Compose                       |

---

## ğŸ“ Project Structure

```bash
metadata-etl-pipeline/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ kafka/
â”‚   â””â”€â”€ connect/
â”‚       â””â”€â”€ postgres-connector.json
â”œâ”€â”€ metadata-db/
â”‚   â””â”€â”€ init/
â”‚       â””â”€â”€ init.sql
â”œâ”€â”€ spark/
â”‚   â””â”€â”€ etl_job.py
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ metadata_etl_dag.py
â”œâ”€â”€ envs/
â”‚   â””â”€â”€ snowflake.env
â””â”€â”€ README.md



 How It Works
1. Metadata Table in PostgreSQL
Stores transformation rules:

sql
Copy
Edit
CREATE TABLE etl_rules (
  source_db VARCHAR,
  table_name VARCHAR,
  column_name VARCHAR,
  transformation_rule TEXT,
  target_db VARCHAR,
  target_table VARCHAR
);
2. CDC via Kafka + Debezium
Debezium connectors capture changes from PostgreSQL and SQL Server and publish them to Kafka topics.

3. Transformation via PySpark
Spark reads Kafka streams and applies transformations based on the metadata rules from PostgreSQL.

4. Load to Destinations
Transformed data is loaded into:

MySQL for dashboarding

Snowflake for analytics and data warehousing

5. Airflow DAG
Orchestrates the ETL job and monitors executions.

6. Power BI
Connects directly to MySQL and Snowflake to build dashboards.
